{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "RNN is a type of artificial neural network in which the hidden node is connected to the directional edge to form a circulating structure. It is known as a model suitable for processing data that appears sequentially, such as voice and text. \n",
    "\n",
    "As shown in the figure above, RNN's greatest advantage is its ability to create flexible and flexible structures as needed, as it can accept inputs and outputs regardless of sequence length.\n",
    "\n",
    "![RNN_architecture](../../../image/rnn_arch.png)\n",
    "\n",
    "## RNN`s basic structure\n",
    "![RNN_basic](../../../image/s8nYcww.png)\n",
    "The basic structure of the RNN is shown in the figure above. A green box means a hidden state. The red box is the input x, the blue box is the output y. The current state's hidden state ht is updated with the previous hidden state ht-1.\n",
    "\n",
    "The output yt of the current state is a structure that is updated by receiving ht. As the formula shows, the activation function of the hidden state is a hyperbolic tangent (tanh), which is a nonlinear function.\n",
    "\n",
    "## RNN`s forward Propagation\n",
    "![RNN_forward](../../../image/TIdBDTJ.png)\n",
    "\n",
    "## RNN`s backward Propagation\n",
    "![RNN backpropagtion](../../../image/XYDxsNs.png)\n",
    "## Reference \n",
    "https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "num_epochs = 2\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                           train=True,\n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                          train=False,\n",
    "                                          transform=transforms.ToTensor()\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Neural Network (many-to-one)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_layers,num_classes):\n",
    "        super(RNN,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size,hidden_size,num_layers,batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size,num_classes)\n",
    "    \n",
    "    def forward(self,x) :\n",
    "        # set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers,x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers,x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out,_ = self.lstm(x,(h0,c0)) # out : tensor of shape (batch,seq_length,hidden_size)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        return out\n",
    "    \n",
    "model = RNN(input_size,hidden_size,num_layers,num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss, optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [100/600], Loss: 0.3794\n",
      "Epoch [1/2], Step [200/600], Loss: 0.4424\n",
      "Epoch [1/2], Step [300/600], Loss: 0.2403\n",
      "Epoch [1/2], Step [400/600], Loss: 0.1716\n",
      "Epoch [1/2], Step [500/600], Loss: 0.1249\n",
      "Epoch [1/2], Step [600/600], Loss: 0.1374\n",
      "Epoch [2/2], Step [100/600], Loss: 0.1075\n",
      "Epoch [2/2], Step [200/600], Loss: 0.0668\n",
      "Epoch [2/2], Step [300/600], Loss: 0.1059\n",
      "Epoch [2/2], Step [400/600], Loss: 0.0399\n",
      "Epoch [2/2], Step [500/600], Loss: 0.0180\n",
      "Epoch [2/2], Step [600/600], Loss: 0.1150\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "total_step = len(train_loader) \n",
    "for epoch in range(num_epochs) :\n",
    "    for i, (images,labels) in enumerate(train_loader) :\n",
    "        images = images.reshape(-1,sequence_length,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs,labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 97.78 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'rnn.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
