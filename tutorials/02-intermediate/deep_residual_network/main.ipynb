{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "# An implementation of https://arxiv.org/pdf/1512.03385.pdf                    #\n",
    "# See section 4.2 for the model architecture on CIFAR-10                       #\n",
    "# Some part of the code was referenced from below                              #\n",
    "# https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py   #\n",
    "# ---------------------------------------------------------------------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Residual Network](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "\n",
    "## Problem of deeper layer\n",
    "### 1. Vanishing / Exploding Gradient\n",
    "When you update a parameter in CNN, the gradient value is saturated with a too large value or a small value, and it does not move anymore, so the learning effect disappears or the learning speed becomes very slow. \n",
    "\n",
    "As the network becomes deeper, this problem becomes more and more serious.\n",
    "\n",
    "In order to avoid this problem, techniques such as batch normalization and parameter initialization are applied.\n",
    "\n",
    "However, if the number of layers exceeds a certain number, it still becomes a headache .\n",
    "\n",
    "### 2. Diffcult to learning \n",
    "If the network is deepened, the number of parameters will increase proportionally, and even though it is not a problem of overfitting, a situation occurs in which the error becomes rather large.\n",
    "\n",
    "## Residual block\n",
    "In CNN, learning is performed to obtain H(x), but in resnet, learning is performed to obtain H(x)-x (F(x)=H(x)+x). </p> In the optimal case, F(x) must be zero, so the direction to learn is predetermined and this is the pre-conditioning role. \n",
    "\n",
    "If learning is performed in a direction in which F(x) becomes almost zero, a small fluctuation of the input can be easily detected. \n",
    "\n",
    "In this sense, F(x) is called residual learning in terms of learning a small motion, the residual.\n",
    "\n",
    "Also, since the same x as the input is connected to the output as it is, there is no effect on the number of parameters, and there is no increase in the computation through the shortcut connection except for the addition. \n",
    "\n",
    "Since input and output are connected by skipping several layers, forward and backward paths can be simplified.\n",
    "\n",
    "\n",
    "As a result, the effect that can be obtained through the connection of the identity shortcut is as follows :\n",
    "1. Deep layers can be easily optimized.\n",
    "\n",
    "2. Increased depth can improve accuracy.\n",
    "![Residual block](https://cdn-images-1.medium.com/max/1200/1*ByrVJspW-TefwlH7OLxNkg.png)\n",
    "\n",
    "\n",
    "## VGG,Plain,Residual Architecture\n",
    "![Residual Network](https://cdn-images-1.medium.com/max/1200/1*2ns4ota94je5gSVjrpFq3A.png)\n",
    "\n",
    "## Deeper Bottleneck Architecture\n",
    "Considering the training time, the basic structure is slightly modified for 50- / 101- / 152-layer, and the residual function is composed of 1x1, 3x3, 1x1. \n",
    "\n",
    "The reason named Bottleneck structure is because it looks like a bottleneck in reducing dimensions and increasing dimensions from the back.\n",
    "\n",
    "The reason for this configuration is to reduce computation time.\n",
    "\n",
    "The first 1x1 convolution is intended to reduce the dimension as shown in the Inception structure of NIN (Network-in-Network) or GoogLeNet. \n",
    "\n",
    "After reducing the dimension and performing the 3x3 convolution, the final 1x1 convolution enlarges the dimension again. \n",
    "\n",
    "It plays a role. As a result, it is possible to reduce the computational complexity as compared with the structure in which two 3x3 convolutions are directly connected.\n",
    "\n",
    "![bottleneck](https://camo.qiitausercontent.com/1bb6b91505493e7e08fc0c9430f2ce2978e3bb70/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3130303532332f37346438383965662d386462642d363365632d336661302d3631396533663232366537312e706e67)\n",
    "\n",
    "## Reference\n",
    "https://laonple.blog.me/220761052425 </BR>\n",
    "https://laonple.blog.me/220764986252 </BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device configuration\n",
    "device = torch.device('cuda:1')\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-paramters\n",
    "num_epochs = 80\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing modules\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transforms.Pad()\n",
    "Pad the given PIL Image on all sides with the given “pad” value. \n",
    "\n",
    "### transforms.RandomHorizonFlip()\n",
    "Horizontally flip the given PIL Image randomly with a given probability.(default p = 0.5)\n",
    "\n",
    "### transforms.RandomCrop()\n",
    "Crop the given PIL Image at a random location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
    "                                             train=True,\n",
    "                                             transform=transform,\n",
    "                                             download=True)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
    "                                            train=False,\n",
    "                                            transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100,\n",
    "                                           shuffle=True) \n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=100,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3x3 convolution\n",
    "def conv3x3layer(in_channels,out_channels,stride=1) :\n",
    "    conv_layer = nn.Conv2d(in_channels,out_channels,kernel_size=3,\n",
    "                        stride=stride,padding=1,bias=False)\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual block\n",
    "class ResidualBlock(nn.Module) :\n",
    "    def __init__(self,in_channels,out_channels,stride=1,downsample=None) :\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3layer(in_channels,out_channels,stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3layer(out_channels,out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "    def forward(self,x) :\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample :\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 14-layer ResNet model\n",
    "![cifar10_resnet](../../../image/image_54.png)\n",
    "\n",
    "## CIFAR-10,100 ResNet result\n",
    "![cifar result](../../../image/image_55.png)\n",
    "\n",
    "## Reference\n",
    "https://laonple.blog.me/220770760226"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet\n",
    "class ResNet(nn.Module) :\n",
    "    def __init__(self,block,layers,num_classes=10) :\n",
    "        super(ResNet,self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3layer(3,16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block,16,layers[0])\n",
    "        self.layer2 = self.make_layer(block,32,layers[1],2)\n",
    "        self.layer3 = self.make_layer(block,64,layers[2],2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64,num_classes)\n",
    "    def make_layer(self,block,out_channels,blocks,stride=1) :\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels) :\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3layer(self.in_channels,out_channels,stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels,out_channels,stride,downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1,blocks) :\n",
    "            layers.append(block(out_channels,out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self,x) :\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0),-1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "model = ResNet(ResidualBlock,[2,2,2]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For updating learning rate\n",
    "def update_lr(optimizer,lr) :\n",
    "    for param_group in optimizer.param_groups :\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Step [100/500] Loss: 1.7200\n",
      "Epoch [1/80], Step [200/500] Loss: 1.4032\n",
      "Epoch [1/80], Step [300/500] Loss: 1.3187\n",
      "Epoch [1/80], Step [400/500] Loss: 1.3043\n",
      "Epoch [1/80], Step [500/500] Loss: 1.1406\n",
      "Epoch [2/80], Step [100/500] Loss: 1.1065\n",
      "Epoch [2/80], Step [200/500] Loss: 1.0044\n",
      "Epoch [2/80], Step [300/500] Loss: 0.9449\n",
      "Epoch [2/80], Step [400/500] Loss: 1.0289\n",
      "Epoch [2/80], Step [500/500] Loss: 0.9420\n",
      "Epoch [3/80], Step [100/500] Loss: 1.0902\n",
      "Epoch [3/80], Step [200/500] Loss: 1.0721\n",
      "Epoch [3/80], Step [300/500] Loss: 0.8130\n",
      "Epoch [3/80], Step [400/500] Loss: 0.9076\n",
      "Epoch [3/80], Step [500/500] Loss: 0.9649\n",
      "Epoch [4/80], Step [100/500] Loss: 0.7986\n",
      "Epoch [4/80], Step [200/500] Loss: 0.7498\n",
      "Epoch [4/80], Step [300/500] Loss: 0.9079\n",
      "Epoch [4/80], Step [400/500] Loss: 0.8767\n",
      "Epoch [4/80], Step [500/500] Loss: 0.7828\n",
      "Epoch [5/80], Step [100/500] Loss: 0.8237\n",
      "Epoch [5/80], Step [200/500] Loss: 0.7378\n",
      "Epoch [5/80], Step [300/500] Loss: 0.8489\n",
      "Epoch [5/80], Step [400/500] Loss: 0.6612\n",
      "Epoch [5/80], Step [500/500] Loss: 0.6685\n",
      "Epoch [6/80], Step [100/500] Loss: 0.7754\n",
      "Epoch [6/80], Step [200/500] Loss: 0.9348\n",
      "Epoch [6/80], Step [300/500] Loss: 0.6635\n",
      "Epoch [6/80], Step [400/500] Loss: 0.4640\n",
      "Epoch [6/80], Step [500/500] Loss: 0.4816\n",
      "Epoch [7/80], Step [100/500] Loss: 0.8559\n",
      "Epoch [7/80], Step [200/500] Loss: 0.5441\n",
      "Epoch [7/80], Step [300/500] Loss: 0.5663\n",
      "Epoch [7/80], Step [400/500] Loss: 0.6200\n",
      "Epoch [7/80], Step [500/500] Loss: 0.6417\n",
      "Epoch [8/80], Step [100/500] Loss: 0.7009\n",
      "Epoch [8/80], Step [200/500] Loss: 0.5577\n",
      "Epoch [8/80], Step [300/500] Loss: 0.5601\n",
      "Epoch [8/80], Step [400/500] Loss: 0.6948\n",
      "Epoch [8/80], Step [500/500] Loss: 0.5000\n",
      "Epoch [9/80], Step [100/500] Loss: 0.5700\n",
      "Epoch [9/80], Step [200/500] Loss: 0.7024\n",
      "Epoch [9/80], Step [300/500] Loss: 0.6920\n",
      "Epoch [9/80], Step [400/500] Loss: 0.5580\n",
      "Epoch [9/80], Step [500/500] Loss: 0.4003\n",
      "Epoch [10/80], Step [100/500] Loss: 0.6255\n",
      "Epoch [10/80], Step [200/500] Loss: 0.4047\n",
      "Epoch [10/80], Step [300/500] Loss: 0.5034\n",
      "Epoch [10/80], Step [400/500] Loss: 0.3368\n",
      "Epoch [10/80], Step [500/500] Loss: 0.5494\n",
      "Epoch [11/80], Step [100/500] Loss: 0.5284\n",
      "Epoch [11/80], Step [200/500] Loss: 0.4598\n",
      "Epoch [11/80], Step [300/500] Loss: 0.4887\n",
      "Epoch [11/80], Step [400/500] Loss: 0.6590\n",
      "Epoch [11/80], Step [500/500] Loss: 0.4922\n",
      "Epoch [12/80], Step [100/500] Loss: 0.4166\n",
      "Epoch [12/80], Step [200/500] Loss: 0.5520\n",
      "Epoch [12/80], Step [300/500] Loss: 0.4550\n",
      "Epoch [12/80], Step [400/500] Loss: 0.3558\n",
      "Epoch [12/80], Step [500/500] Loss: 0.3283\n",
      "Epoch [13/80], Step [100/500] Loss: 0.4759\n",
      "Epoch [13/80], Step [200/500] Loss: 0.4695\n",
      "Epoch [13/80], Step [300/500] Loss: 0.5692\n",
      "Epoch [13/80], Step [400/500] Loss: 0.3558\n",
      "Epoch [13/80], Step [500/500] Loss: 0.3440\n",
      "Epoch [14/80], Step [100/500] Loss: 0.2972\n",
      "Epoch [14/80], Step [200/500] Loss: 0.2542\n",
      "Epoch [14/80], Step [300/500] Loss: 0.4215\n",
      "Epoch [14/80], Step [400/500] Loss: 0.3707\n",
      "Epoch [14/80], Step [500/500] Loss: 0.4626\n",
      "Epoch [15/80], Step [100/500] Loss: 0.4497\n",
      "Epoch [15/80], Step [200/500] Loss: 0.4423\n",
      "Epoch [15/80], Step [300/500] Loss: 0.3944\n",
      "Epoch [15/80], Step [400/500] Loss: 0.4375\n",
      "Epoch [15/80], Step [500/500] Loss: 0.5835\n",
      "Epoch [16/80], Step [100/500] Loss: 0.3754\n",
      "Epoch [16/80], Step [200/500] Loss: 0.3966\n",
      "Epoch [16/80], Step [300/500] Loss: 0.3609\n",
      "Epoch [16/80], Step [400/500] Loss: 0.4128\n",
      "Epoch [16/80], Step [500/500] Loss: 0.4244\n",
      "Epoch [17/80], Step [100/500] Loss: 0.3708\n",
      "Epoch [17/80], Step [200/500] Loss: 0.5167\n",
      "Epoch [17/80], Step [300/500] Loss: 0.3293\n",
      "Epoch [17/80], Step [400/500] Loss: 0.3365\n",
      "Epoch [17/80], Step [500/500] Loss: 0.2935\n",
      "Epoch [18/80], Step [100/500] Loss: 0.3620\n",
      "Epoch [18/80], Step [200/500] Loss: 0.2693\n",
      "Epoch [18/80], Step [300/500] Loss: 0.4379\n",
      "Epoch [18/80], Step [400/500] Loss: 0.3657\n",
      "Epoch [18/80], Step [500/500] Loss: 0.4948\n",
      "Epoch [19/80], Step [100/500] Loss: 0.3087\n",
      "Epoch [19/80], Step [200/500] Loss: 0.4533\n",
      "Epoch [19/80], Step [300/500] Loss: 0.5331\n",
      "Epoch [19/80], Step [400/500] Loss: 0.4544\n",
      "Epoch [19/80], Step [500/500] Loss: 0.3907\n",
      "Epoch [20/80], Step [100/500] Loss: 0.2468\n",
      "Epoch [20/80], Step [200/500] Loss: 0.4722\n",
      "Epoch [20/80], Step [300/500] Loss: 0.3314\n",
      "Epoch [20/80], Step [400/500] Loss: 0.3027\n",
      "Epoch [20/80], Step [500/500] Loss: 0.3509\n",
      "Epoch [21/80], Step [100/500] Loss: 0.3101\n",
      "Epoch [21/80], Step [200/500] Loss: 0.3974\n",
      "Epoch [21/80], Step [300/500] Loss: 0.3111\n",
      "Epoch [21/80], Step [400/500] Loss: 0.3493\n",
      "Epoch [21/80], Step [500/500] Loss: 0.3743\n",
      "Epoch [22/80], Step [100/500] Loss: 0.2853\n",
      "Epoch [22/80], Step [200/500] Loss: 0.2304\n",
      "Epoch [22/80], Step [300/500] Loss: 0.3376\n",
      "Epoch [22/80], Step [400/500] Loss: 0.4482\n",
      "Epoch [22/80], Step [500/500] Loss: 0.3389\n",
      "Epoch [23/80], Step [100/500] Loss: 0.4075\n",
      "Epoch [23/80], Step [200/500] Loss: 0.3736\n",
      "Epoch [23/80], Step [300/500] Loss: 0.3959\n",
      "Epoch [23/80], Step [400/500] Loss: 0.2763\n",
      "Epoch [23/80], Step [500/500] Loss: 0.5257\n",
      "Epoch [24/80], Step [100/500] Loss: 0.3643\n",
      "Epoch [24/80], Step [200/500] Loss: 0.2338\n",
      "Epoch [24/80], Step [300/500] Loss: 0.2781\n",
      "Epoch [24/80], Step [400/500] Loss: 0.5057\n",
      "Epoch [24/80], Step [500/500] Loss: 0.3910\n",
      "Epoch [25/80], Step [100/500] Loss: 0.6146\n",
      "Epoch [25/80], Step [200/500] Loss: 0.2519\n",
      "Epoch [25/80], Step [300/500] Loss: 0.5352\n",
      "Epoch [25/80], Step [400/500] Loss: 0.3179\n",
      "Epoch [25/80], Step [500/500] Loss: 0.4066\n",
      "Epoch [26/80], Step [100/500] Loss: 0.4016\n",
      "Epoch [26/80], Step [200/500] Loss: 0.3288\n",
      "Epoch [26/80], Step [300/500] Loss: 0.4269\n",
      "Epoch [26/80], Step [400/500] Loss: 0.4341\n",
      "Epoch [26/80], Step [500/500] Loss: 0.3667\n",
      "Epoch [27/80], Step [100/500] Loss: 0.4991\n",
      "Epoch [27/80], Step [200/500] Loss: 0.3283\n",
      "Epoch [27/80], Step [300/500] Loss: 0.3513\n",
      "Epoch [27/80], Step [400/500] Loss: 0.2747\n",
      "Epoch [27/80], Step [500/500] Loss: 0.4243\n",
      "Epoch [28/80], Step [100/500] Loss: 0.3861\n",
      "Epoch [28/80], Step [200/500] Loss: 0.3105\n",
      "Epoch [28/80], Step [300/500] Loss: 0.4743\n",
      "Epoch [28/80], Step [400/500] Loss: 0.3998\n",
      "Epoch [28/80], Step [500/500] Loss: 0.4218\n",
      "Epoch [29/80], Step [100/500] Loss: 0.4498\n",
      "Epoch [29/80], Step [200/500] Loss: 0.3677\n",
      "Epoch [29/80], Step [300/500] Loss: 0.3512\n",
      "Epoch [29/80], Step [400/500] Loss: 0.4812\n",
      "Epoch [29/80], Step [500/500] Loss: 0.4296\n",
      "Epoch [30/80], Step [100/500] Loss: 0.3964\n",
      "Epoch [30/80], Step [200/500] Loss: 0.3056\n",
      "Epoch [30/80], Step [300/500] Loss: 0.3789\n",
      "Epoch [30/80], Step [400/500] Loss: 0.2853\n",
      "Epoch [30/80], Step [500/500] Loss: 0.2721\n",
      "Epoch [31/80], Step [100/500] Loss: 0.4193\n",
      "Epoch [31/80], Step [200/500] Loss: 0.3323\n",
      "Epoch [31/80], Step [300/500] Loss: 0.3288\n",
      "Epoch [31/80], Step [400/500] Loss: 0.4812\n",
      "Epoch [31/80], Step [500/500] Loss: 0.3233\n",
      "Epoch [32/80], Step [100/500] Loss: 0.4836\n",
      "Epoch [32/80], Step [200/500] Loss: 0.4176\n",
      "Epoch [32/80], Step [300/500] Loss: 0.3598\n",
      "Epoch [32/80], Step [400/500] Loss: 0.3388\n",
      "Epoch [32/80], Step [500/500] Loss: 0.2564\n",
      "Epoch [33/80], Step [100/500] Loss: 0.3718\n",
      "Epoch [33/80], Step [200/500] Loss: 0.2741\n",
      "Epoch [33/80], Step [300/500] Loss: 0.3854\n",
      "Epoch [33/80], Step [400/500] Loss: 0.5165\n",
      "Epoch [33/80], Step [500/500] Loss: 0.3269\n",
      "Epoch [34/80], Step [100/500] Loss: 0.3823\n",
      "Epoch [34/80], Step [200/500] Loss: 0.3807\n",
      "Epoch [34/80], Step [300/500] Loss: 0.3814\n",
      "Epoch [34/80], Step [400/500] Loss: 0.3700\n",
      "Epoch [34/80], Step [500/500] Loss: 0.2810\n",
      "Epoch [35/80], Step [100/500] Loss: 0.3601\n",
      "Epoch [35/80], Step [200/500] Loss: 0.3917\n",
      "Epoch [35/80], Step [300/500] Loss: 0.3969\n",
      "Epoch [35/80], Step [400/500] Loss: 0.2875\n",
      "Epoch [35/80], Step [500/500] Loss: 0.3705\n",
      "Epoch [36/80], Step [100/500] Loss: 0.1995\n",
      "Epoch [36/80], Step [200/500] Loss: 0.4209\n",
      "Epoch [36/80], Step [300/500] Loss: 0.5552\n",
      "Epoch [36/80], Step [400/500] Loss: 0.3879\n",
      "Epoch [36/80], Step [500/500] Loss: 0.2599\n",
      "Epoch [37/80], Step [100/500] Loss: 0.4406\n",
      "Epoch [37/80], Step [200/500] Loss: 0.3858\n",
      "Epoch [37/80], Step [300/500] Loss: 0.3338\n",
      "Epoch [37/80], Step [400/500] Loss: 0.5027\n",
      "Epoch [37/80], Step [500/500] Loss: 0.3815\n",
      "Epoch [38/80], Step [100/500] Loss: 0.4315\n",
      "Epoch [38/80], Step [200/500] Loss: 0.4649\n",
      "Epoch [38/80], Step [300/500] Loss: 0.4759\n",
      "Epoch [38/80], Step [400/500] Loss: 0.4603\n",
      "Epoch [38/80], Step [500/500] Loss: 0.4620\n",
      "Epoch [39/80], Step [100/500] Loss: 0.3909\n",
      "Epoch [39/80], Step [200/500] Loss: 0.3710\n",
      "Epoch [39/80], Step [300/500] Loss: 0.3780\n",
      "Epoch [39/80], Step [400/500] Loss: 0.4919\n",
      "Epoch [39/80], Step [500/500] Loss: 0.3634\n",
      "Epoch [40/80], Step [100/500] Loss: 0.2915\n",
      "Epoch [40/80], Step [200/500] Loss: 0.2469\n",
      "Epoch [40/80], Step [300/500] Loss: 0.3731\n",
      "Epoch [40/80], Step [400/500] Loss: 0.3421\n",
      "Epoch [40/80], Step [500/500] Loss: 0.3719\n",
      "Epoch [41/80], Step [100/500] Loss: 0.4254\n",
      "Epoch [41/80], Step [200/500] Loss: 0.4144\n",
      "Epoch [41/80], Step [300/500] Loss: 0.3924\n",
      "Epoch [41/80], Step [400/500] Loss: 0.3437\n",
      "Epoch [41/80], Step [500/500] Loss: 0.3848\n",
      "Epoch [42/80], Step [100/500] Loss: 0.4189\n",
      "Epoch [42/80], Step [200/500] Loss: 0.3916\n",
      "Epoch [42/80], Step [300/500] Loss: 0.4037\n",
      "Epoch [42/80], Step [400/500] Loss: 0.4061\n",
      "Epoch [42/80], Step [500/500] Loss: 0.3596\n",
      "Epoch [43/80], Step [100/500] Loss: 0.3811\n",
      "Epoch [43/80], Step [200/500] Loss: 0.3256\n",
      "Epoch [43/80], Step [300/500] Loss: 0.2815\n",
      "Epoch [43/80], Step [400/500] Loss: 0.3804\n",
      "Epoch [43/80], Step [500/500] Loss: 0.4846\n",
      "Epoch [44/80], Step [100/500] Loss: 0.5417\n",
      "Epoch [44/80], Step [200/500] Loss: 0.5853\n",
      "Epoch [44/80], Step [300/500] Loss: 0.3578\n",
      "Epoch [44/80], Step [400/500] Loss: 0.3675\n",
      "Epoch [44/80], Step [500/500] Loss: 0.3325\n",
      "Epoch [45/80], Step [100/500] Loss: 0.3785\n",
      "Epoch [45/80], Step [200/500] Loss: 0.3672\n",
      "Epoch [45/80], Step [300/500] Loss: 0.5013\n",
      "Epoch [45/80], Step [400/500] Loss: 0.4485\n",
      "Epoch [45/80], Step [500/500] Loss: 0.2748\n",
      "Epoch [46/80], Step [100/500] Loss: 0.3695\n",
      "Epoch [46/80], Step [200/500] Loss: 0.3728\n",
      "Epoch [46/80], Step [300/500] Loss: 0.3628\n",
      "Epoch [46/80], Step [400/500] Loss: 0.4475\n",
      "Epoch [46/80], Step [500/500] Loss: 0.4372\n",
      "Epoch [47/80], Step [100/500] Loss: 0.3378\n",
      "Epoch [47/80], Step [200/500] Loss: 0.3552\n",
      "Epoch [47/80], Step [300/500] Loss: 0.3325\n",
      "Epoch [47/80], Step [400/500] Loss: 0.4124\n",
      "Epoch [47/80], Step [500/500] Loss: 0.4733\n",
      "Epoch [48/80], Step [100/500] Loss: 0.2948\n",
      "Epoch [48/80], Step [200/500] Loss: 0.2560\n",
      "Epoch [48/80], Step [300/500] Loss: 0.4917\n",
      "Epoch [48/80], Step [400/500] Loss: 0.3490\n",
      "Epoch [48/80], Step [500/500] Loss: 0.4884\n",
      "Epoch [49/80], Step [100/500] Loss: 0.3819\n",
      "Epoch [49/80], Step [200/500] Loss: 0.3400\n",
      "Epoch [49/80], Step [300/500] Loss: 0.3883\n",
      "Epoch [49/80], Step [400/500] Loss: 0.2681\n",
      "Epoch [49/80], Step [500/500] Loss: 0.4040\n",
      "Epoch [50/80], Step [100/500] Loss: 0.3485\n",
      "Epoch [50/80], Step [200/500] Loss: 0.2402\n",
      "Epoch [50/80], Step [300/500] Loss: 0.4302\n",
      "Epoch [50/80], Step [400/500] Loss: 0.3771\n",
      "Epoch [50/80], Step [500/500] Loss: 0.3918\n",
      "Epoch [51/80], Step [100/500] Loss: 0.4192\n",
      "Epoch [51/80], Step [200/500] Loss: 0.3823\n",
      "Epoch [51/80], Step [300/500] Loss: 0.3555\n",
      "Epoch [51/80], Step [400/500] Loss: 0.3435\n",
      "Epoch [51/80], Step [500/500] Loss: 0.3918\n",
      "Epoch [52/80], Step [100/500] Loss: 0.4105\n",
      "Epoch [52/80], Step [200/500] Loss: 0.4301\n",
      "Epoch [52/80], Step [300/500] Loss: 0.3862\n",
      "Epoch [52/80], Step [400/500] Loss: 0.4272\n",
      "Epoch [52/80], Step [500/500] Loss: 0.3766\n",
      "Epoch [53/80], Step [100/500] Loss: 0.4994\n",
      "Epoch [53/80], Step [200/500] Loss: 0.3742\n",
      "Epoch [53/80], Step [300/500] Loss: 0.4503\n",
      "Epoch [53/80], Step [400/500] Loss: 0.3950\n",
      "Epoch [53/80], Step [500/500] Loss: 0.3108\n",
      "Epoch [54/80], Step [100/500] Loss: 0.2791\n",
      "Epoch [54/80], Step [200/500] Loss: 0.4213\n",
      "Epoch [54/80], Step [300/500] Loss: 0.4971\n",
      "Epoch [54/80], Step [400/500] Loss: 0.4053\n",
      "Epoch [54/80], Step [500/500] Loss: 0.5345\n",
      "Epoch [55/80], Step [100/500] Loss: 0.4090\n",
      "Epoch [55/80], Step [200/500] Loss: 0.4886\n",
      "Epoch [55/80], Step [300/500] Loss: 0.2990\n",
      "Epoch [55/80], Step [400/500] Loss: 0.3525\n",
      "Epoch [55/80], Step [500/500] Loss: 0.5286\n",
      "Epoch [56/80], Step [100/500] Loss: 0.2943\n",
      "Epoch [56/80], Step [200/500] Loss: 0.4112\n",
      "Epoch [56/80], Step [300/500] Loss: 0.5093\n",
      "Epoch [56/80], Step [400/500] Loss: 0.4095\n",
      "Epoch [56/80], Step [500/500] Loss: 0.2461\n",
      "Epoch [57/80], Step [100/500] Loss: 0.5311\n",
      "Epoch [57/80], Step [200/500] Loss: 0.2952\n",
      "Epoch [57/80], Step [300/500] Loss: 0.2771\n",
      "Epoch [57/80], Step [400/500] Loss: 0.3845\n",
      "Epoch [57/80], Step [500/500] Loss: 0.3841\n",
      "Epoch [58/80], Step [100/500] Loss: 0.4081\n",
      "Epoch [58/80], Step [200/500] Loss: 0.4188\n",
      "Epoch [58/80], Step [300/500] Loss: 0.2845\n",
      "Epoch [58/80], Step [400/500] Loss: 0.4795\n",
      "Epoch [58/80], Step [500/500] Loss: 0.2757\n",
      "Epoch [59/80], Step [100/500] Loss: 0.3853\n",
      "Epoch [59/80], Step [200/500] Loss: 0.3454\n",
      "Epoch [59/80], Step [300/500] Loss: 0.3418\n",
      "Epoch [59/80], Step [400/500] Loss: 0.2708\n",
      "Epoch [59/80], Step [500/500] Loss: 0.3398\n",
      "Epoch [60/80], Step [100/500] Loss: 0.3318\n",
      "Epoch [60/80], Step [200/500] Loss: 0.3631\n",
      "Epoch [60/80], Step [300/500] Loss: 0.3641\n",
      "Epoch [60/80], Step [400/500] Loss: 0.1946\n",
      "Epoch [60/80], Step [500/500] Loss: 0.3722\n",
      "Epoch [61/80], Step [100/500] Loss: 0.5010\n",
      "Epoch [61/80], Step [200/500] Loss: 0.3411\n",
      "Epoch [61/80], Step [300/500] Loss: 0.4732\n",
      "Epoch [61/80], Step [400/500] Loss: 0.2477\n",
      "Epoch [61/80], Step [500/500] Loss: 0.3803\n",
      "Epoch [62/80], Step [100/500] Loss: 0.4666\n",
      "Epoch [62/80], Step [200/500] Loss: 0.3857\n",
      "Epoch [62/80], Step [300/500] Loss: 0.2658\n",
      "Epoch [62/80], Step [400/500] Loss: 0.5014\n",
      "Epoch [62/80], Step [500/500] Loss: 0.4116\n",
      "Epoch [63/80], Step [100/500] Loss: 0.1522\n",
      "Epoch [63/80], Step [200/500] Loss: 0.2482\n",
      "Epoch [63/80], Step [300/500] Loss: 0.4239\n",
      "Epoch [63/80], Step [400/500] Loss: 0.3472\n",
      "Epoch [63/80], Step [500/500] Loss: 0.3734\n",
      "Epoch [64/80], Step [100/500] Loss: 0.3321\n",
      "Epoch [64/80], Step [200/500] Loss: 0.2820\n",
      "Epoch [64/80], Step [300/500] Loss: 0.4717\n",
      "Epoch [64/80], Step [400/500] Loss: 0.3620\n",
      "Epoch [64/80], Step [500/500] Loss: 0.3487\n",
      "Epoch [65/80], Step [100/500] Loss: 0.4757\n",
      "Epoch [65/80], Step [200/500] Loss: 0.3515\n",
      "Epoch [65/80], Step [300/500] Loss: 0.4215\n",
      "Epoch [65/80], Step [400/500] Loss: 0.4070\n",
      "Epoch [65/80], Step [500/500] Loss: 0.5055\n",
      "Epoch [66/80], Step [100/500] Loss: 0.5063\n",
      "Epoch [66/80], Step [200/500] Loss: 0.3531\n",
      "Epoch [66/80], Step [300/500] Loss: 0.3407\n",
      "Epoch [66/80], Step [400/500] Loss: 0.3963\n",
      "Epoch [66/80], Step [500/500] Loss: 0.4004\n",
      "Epoch [67/80], Step [100/500] Loss: 0.4656\n",
      "Epoch [67/80], Step [200/500] Loss: 0.5152\n",
      "Epoch [67/80], Step [300/500] Loss: 0.2799\n",
      "Epoch [67/80], Step [400/500] Loss: 0.3809\n",
      "Epoch [67/80], Step [500/500] Loss: 0.4544\n",
      "Epoch [68/80], Step [100/500] Loss: 0.2970\n",
      "Epoch [68/80], Step [200/500] Loss: 0.3651\n",
      "Epoch [68/80], Step [300/500] Loss: 0.4774\n",
      "Epoch [68/80], Step [400/500] Loss: 0.3787\n",
      "Epoch [68/80], Step [500/500] Loss: 0.3472\n",
      "Epoch [69/80], Step [100/500] Loss: 0.3727\n",
      "Epoch [69/80], Step [200/500] Loss: 0.3514\n",
      "Epoch [69/80], Step [300/500] Loss: 0.3048\n",
      "Epoch [69/80], Step [400/500] Loss: 0.3704\n",
      "Epoch [69/80], Step [500/500] Loss: 0.3574\n",
      "Epoch [70/80], Step [100/500] Loss: 0.2856\n",
      "Epoch [70/80], Step [200/500] Loss: 0.5409\n",
      "Epoch [70/80], Step [300/500] Loss: 0.3746\n",
      "Epoch [70/80], Step [400/500] Loss: 0.3854\n",
      "Epoch [70/80], Step [500/500] Loss: 0.3333\n",
      "Epoch [71/80], Step [100/500] Loss: 0.2904\n",
      "Epoch [71/80], Step [200/500] Loss: 0.3757\n",
      "Epoch [71/80], Step [300/500] Loss: 0.3708\n",
      "Epoch [71/80], Step [400/500] Loss: 0.3489\n",
      "Epoch [71/80], Step [500/500] Loss: 0.5296\n",
      "Epoch [72/80], Step [100/500] Loss: 0.3737\n",
      "Epoch [72/80], Step [200/500] Loss: 0.5409\n",
      "Epoch [72/80], Step [300/500] Loss: 0.4756\n",
      "Epoch [72/80], Step [400/500] Loss: 0.3475\n",
      "Epoch [72/80], Step [500/500] Loss: 0.4557\n",
      "Epoch [73/80], Step [100/500] Loss: 0.3437\n",
      "Epoch [73/80], Step [200/500] Loss: 0.4281\n",
      "Epoch [73/80], Step [300/500] Loss: 0.4577\n",
      "Epoch [73/80], Step [400/500] Loss: 0.2486\n",
      "Epoch [73/80], Step [500/500] Loss: 0.4227\n",
      "Epoch [74/80], Step [100/500] Loss: 0.3181\n",
      "Epoch [74/80], Step [200/500] Loss: 0.3969\n",
      "Epoch [74/80], Step [300/500] Loss: 0.4102\n",
      "Epoch [74/80], Step [400/500] Loss: 0.3950\n",
      "Epoch [74/80], Step [500/500] Loss: 0.3915\n",
      "Epoch [75/80], Step [100/500] Loss: 0.4355\n",
      "Epoch [75/80], Step [200/500] Loss: 0.2775\n",
      "Epoch [75/80], Step [300/500] Loss: 0.3772\n",
      "Epoch [75/80], Step [400/500] Loss: 0.4272\n",
      "Epoch [75/80], Step [500/500] Loss: 0.3967\n",
      "Epoch [76/80], Step [100/500] Loss: 0.4144\n",
      "Epoch [76/80], Step [200/500] Loss: 0.5007\n",
      "Epoch [76/80], Step [300/500] Loss: 0.4665\n",
      "Epoch [76/80], Step [400/500] Loss: 0.4176\n",
      "Epoch [76/80], Step [500/500] Loss: 0.3334\n",
      "Epoch [77/80], Step [100/500] Loss: 0.3122\n",
      "Epoch [77/80], Step [200/500] Loss: 0.3842\n",
      "Epoch [77/80], Step [300/500] Loss: 0.3859\n",
      "Epoch [77/80], Step [400/500] Loss: 0.2694\n",
      "Epoch [77/80], Step [500/500] Loss: 0.3593\n",
      "Epoch [78/80], Step [100/500] Loss: 0.4544\n",
      "Epoch [78/80], Step [200/500] Loss: 0.3562\n",
      "Epoch [78/80], Step [300/500] Loss: 0.2975\n",
      "Epoch [78/80], Step [400/500] Loss: 0.3961\n",
      "Epoch [78/80], Step [500/500] Loss: 0.5199\n",
      "Epoch [79/80], Step [100/500] Loss: 0.3803\n",
      "Epoch [79/80], Step [200/500] Loss: 0.2981\n",
      "Epoch [79/80], Step [300/500] Loss: 0.4077\n",
      "Epoch [79/80], Step [400/500] Loss: 0.5771\n",
      "Epoch [79/80], Step [500/500] Loss: 0.5520\n",
      "Epoch [80/80], Step [100/500] Loss: 0.3607\n",
      "Epoch [80/80], Step [200/500] Loss: 0.4453\n",
      "Epoch [80/80], Step [300/500] Loss: 0.4324\n",
      "Epoch [80/80], Step [400/500] Loss: 0.4170\n",
      "Epoch [80/80], Step [500/500] Loss: 0.4054\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "curr_lr = learning_rate\n",
    "for epoch in range(num_epochs) :\n",
    "    for i, (images,labels) in enumerate(train_loader) :\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs,labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "        \n",
    "        # Decay learning rate\n",
    "        if (epoch+1) % 20 == 0:\n",
    "            curr_lr /= 3\n",
    "            update_lr(optimizer,curr_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 83.82 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'resnet.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
